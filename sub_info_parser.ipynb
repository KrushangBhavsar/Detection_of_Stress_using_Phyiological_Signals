{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sub_info_parser.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpeNMQEgIAo1"
      },
      "source": [
        "#Parsing Important details of subject such as diet,lifestyle,age,height,width\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "class rparser:\n",
        "    # Code adapted from https://github.com/arsen-movsesyan/springboard_WESAD/blob/master/parsers/readme_parser.py\n",
        "    VALUE_EXTRACT_KEYS = {\n",
        "        \"age\": {\n",
        "            'search_key': 'Age',\n",
        "            'delimiter': ':'\n",
        "        },\n",
        "        \"height\": {\n",
        "            'search_key': 'Height',\n",
        "            'delimiter': ':'\n",
        "        },\n",
        "        \"weight\": {\n",
        "            'search_key': 'Weight',\n",
        "            'delimiter': ':'\n",
        "        },\n",
        "        \"gender\": {\n",
        "            'search_key': 'Gender',\n",
        "            'delimiter': ':'\n",
        "        },\n",
        "        \"dominant_hand\": {\n",
        "            'search_key': 'Dominant',\n",
        "            'delimiter': ':'\n",
        "        },\n",
        "        \"coffee_today\": {\n",
        "            'search_key': 'Did you drink coffee today',\n",
        "            'delimiter': '? '\n",
        "        },\n",
        "        \"coffee_last_hour\": {\n",
        "            'search_key': 'Did you drink coffee within the last hour',\n",
        "            'delimiter': '? '\n",
        "        },\n",
        "        \"sport_today\": {\n",
        "            'search_key': 'Did you do any sports today',\n",
        "            'delimiter': '? '\n",
        "        },\n",
        "        \"smoker\": {\n",
        "            'search_key': 'Are you a smoker',\n",
        "            'delimiter': '? '\n",
        "        },\n",
        "        \"smoke_last_hour\": {\n",
        "            'search_key': 'Did you smoke within the last hour',\n",
        "            'delimiter': '? '\n",
        "        },\n",
        "        \"feel_ill_today\": {\n",
        "            'search_key': 'Do you feel ill today',\n",
        "            'delimiter': '? '\n",
        "        }\n",
        "    }\n",
        "    # Add your path to Dataset\n",
        "    DATA_PATH = 'Path to Dataset'\n",
        "    parse_file_suffix = '_readme.txt'\n",
        "    \n",
        "    \n",
        "    def __init__(self):\n",
        "        \n",
        "        self.readme_locations = {subject_directory: self.DATA_PATH + subject_directory + '/' \n",
        "                          for subject_directory in os.listdir(self.DATA_PATH)\n",
        "                              if re.match('^S[0-9]{1,2}$', subject_directory)}\n",
        "        \n",
        "        # Check if parsed readme file is available ( should be as it is saved above )\n",
        "        if not os.path.isfile('data/readmes.csv'):\n",
        "            print('Parsing Readme files')\n",
        "            self.parse_all_readmes()\n",
        "        else:\n",
        "            print('Files already parsed.')\n",
        "            \n",
        "        self.merge_with_feature_data()\n",
        "        \n",
        "        \n",
        "    def parse_readme(self, subject_id):\n",
        "        with open(self.readme_locations[subject_id] + subject_id + self.parse_file_suffix, 'r') as f:\n",
        "\n",
        "            x = f.read().split('\\n')\n",
        "\n",
        "        readme_dict = {}\n",
        "\n",
        "        for item in x:\n",
        "            for key in self.VALUE_EXTRACT_KEYS.keys():\n",
        "                search_key = self.VALUE_EXTRACT_KEYS[key]['search_key']\n",
        "                delimiter = self.VALUE_EXTRACT_KEYS[key]['delimiter']\n",
        "                if item.startswith(search_key):\n",
        "                    d, v = item.split(delimiter)\n",
        "                    readme_dict.update({key: v})\n",
        "                    break\n",
        "        return readme_dict\n",
        "\n",
        "\n",
        "    def parse_all_readmes(self):\n",
        "        \n",
        "        dframes = []\n",
        "\n",
        "        for subject_id, path in self.readme_locations.items():\n",
        "            readme_dict = self.parse_readme(subject_id)\n",
        "            df = pd.DataFrame(readme_dict, index=[subject_id])\n",
        "            dframes.append(df)\n",
        "\n",
        "        df = pd.concat(dframes)\n",
        "        df.to_csv(self.DATA_PATH + 'readmes.csv')\n",
        "\n",
        "        \n",
        "    def merge_with_feature_data(self):\n",
        "        # Confirm feature files are available\n",
        "        if os.path.isfile('data/stress.csv'):\n",
        "            feat_df = pd.read_csv('data/stress.csv', index_col=0)\n",
        "        else:\n",
        "            print('No feature data available. Exiting...')\n",
        "            return\n",
        "           \n",
        "        # Combine data and save\n",
        "        df = pd.read_csv(f'{self.DATA_PATH}readmes.csv', index_col=0)\n",
        "\n",
        "        dummy_df = pd.get_dummies(df)\n",
        "        \n",
        "        dummy_df['subject'] = dummy_df.index.str[1:].astype(int)\n",
        "\n",
        "        dummy_df = dummy_df[['age', 'height', 'weight', 'gender_ female', 'gender_ male',\n",
        "                           'coffee_today_YES', 'sport_today_YES', 'smoker_NO', 'smoker_YES',\n",
        "                           'feel_ill_today_YES', 'subject']]\n",
        "\n",
        "        merged_df = pd.merge(feat_df, dummy_df, on='subject')\n",
        "\n",
        "        merged_df.to_csv('data/stress.csv')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  rp = rparser()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}