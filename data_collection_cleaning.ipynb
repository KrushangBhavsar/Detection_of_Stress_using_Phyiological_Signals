{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_collection_cleaning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeTdQDzZHXHW"
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "import scipy.signal as scisig\n",
        "import scipy.stats\n",
        "\n",
        "#Sampling Frequencies\n",
        "smp_frq_dt = {'ACC': 32, 'BVP': 64, 'EDA': 4, 'TEMP': 4, 'label': 700, 'Resp': 700}\n",
        "WINDOW_IN_SECONDS = 30\n",
        "label_dt = {'baseline': 1, 'stress': 2, 'amusement': 0}\n",
        "int_label = {1: 'baseline', 2: 'stress', 0: 'amusement'}\n",
        "feature = None\n",
        "save_path = 'data'\n",
        "sub_feature_path = '/subject_data'\n",
        "\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "if not os.path.exists(save_path + sub_feature_path):\n",
        "    os.makedirs(save_path + sub_feature_path)\n",
        "\n",
        "# cvxEDA\n",
        "def eda_stats(y):\n",
        "    fq = smp_frq_dt['EDA']\n",
        "    yn = (y - y.mean()) / y.std()\n",
        "    [r, p, t, l, d, e, obj] = cvxEDA(yn, 1. / fq)\n",
        "    return [r, p, t, l, d, e, obj]\n",
        "\n",
        "\n",
        "class SubjectData:\n",
        "\n",
        "    def __init__(self, main_path, subject_number):\n",
        "        self.name = f'S{subject_number}'\n",
        "        self.subject = ['signal', 'label', 'subject']\n",
        "        self.signal = ['chest', 'wrist']\n",
        "        self.chest = ['ACC', 'ECG', 'EMG', 'EDA', 'Temp', 'Resp']\n",
        "        self.wrist = ['ACC', 'BVP', 'EDA', 'TEMP']\n",
        "        with open(os.path.join(main_path, self.name) + '/' + self.name + '.pkl', 'rb') as file:\n",
        "            self.data = pickle.load(file, encoding='latin1')\n",
        "        self.labels = self.data['label']\n",
        "\n",
        "    def wrist_data(self):\n",
        "        data = self.data['signal']['wrist']\n",
        "        data.update({'Resp': self.data['signal']['chest']['Resp']})\n",
        "        return data\n",
        "\n",
        "    def chest_data(self):\n",
        "        return self.data['signal']['chest']\n",
        "\n",
        "    def extract_features(self):  # only wrist\n",
        "        results = \\\n",
        "            {\n",
        "                key: get_statistics(self.wrist_data()[key].flatten(), self.labels, key)\n",
        "                for key in self.wrist_keys\n",
        "            }\n",
        "        return results\n",
        "\n",
        "\n",
        "#Lowpass Filter\n",
        "def lowpass(cutoff, fs, order=5):\n",
        "    # Filtering Helper functions\n",
        "    nyquist = 0.5 * fs\n",
        "    normal_cutoff = cutoff / nyquist\n",
        "    b, a = scisig.butter(order, normal_cutoff, btype='low', analog=False)\n",
        "    return b, a\n",
        "\n",
        "#Application of Lowpass Filter\n",
        "def lowpass_filter(data, cutoff, fs, order=5):\n",
        "    # Filtering Helper functions\n",
        "    b, a = lowpass(cutoff, fs, order=order)\n",
        "    y = scisig.lfilter(b, a, data)\n",
        "    return y\n",
        "#Get Slope of Temperature\n",
        "def get_slope(series):\n",
        "    linear_reg = scipy.stats.linregress(np.arange(len(series)), series )\n",
        "    slope = linear_reg[0]\n",
        "    return slope\n",
        "#Compute statistics the Features\n",
        "def compute_window_stats(data, label=-1):\n",
        "    mean_features = np.mean(data)\n",
        "    std_features = np.std(data)\n",
        "    min_features = np.amin(data)\n",
        "    max_features = np.amax(data)\n",
        "\n",
        "    features = {'mean': mean_features, 'std': std_features, 'min': min_features, 'max': max_features,\n",
        "                'label': label}\n",
        "    return features\n",
        "\n",
        "#Three-axis Acceleration\n",
        "def net_accel(data):\n",
        "    return (data['ACC_x'] ** 2 + data['ACC_y'] ** 2 + data['ACC_z'] ** 2).apply(lambda x: np.sqrt(x))\n",
        "\n",
        "#Get Peak Frequency for BVP\n",
        "def peak_freq(x):\n",
        "    frq, peak = scisig.periodogram(x, fs=8)\n",
        "    psd_dict = {amp: freq for amp, freq in zip(peak, frq)}\n",
        "    peak_freq = psd_dict[max(psd_dict.keys())]\n",
        "    return peak_freq\n",
        "\n",
        "\n",
        "# https://github.com/MITMediaLabAffectiveComputing/eda-explorer/blob/master/AccelerometerFeatureExtractionScript.py\n",
        "#Filter Signal for Acceleration\n",
        "def filterSignal(eda, cutoff=0.4, num=64):\n",
        "    f = cutoff / (smp_frq_dt['ACC'] / 2.0)\n",
        "    filter_coeff = scisig.firwin(num, f)\n",
        "\n",
        "    return scisig.lfilter(filter_coeff, 1, eda)\n",
        "\n",
        "\n",
        "def compute_features(e4_data_dict, labels, norm_type=None):\n",
        "    # Dataframes for each sensor type\n",
        "    eda_df = pd.DataFrame(e4_data_dict['EDA'], columns=['EDA'])\n",
        "    bvp_df = pd.DataFrame(e4_data_dict['BVP'], columns=['BVP'])\n",
        "    acc_df = pd.DataFrame(e4_data_dict['ACC'], columns=['ACC_x', 'ACC_y', 'ACC_z'])\n",
        "    temp_df = pd.DataFrame(e4_data_dict['TEMP'], columns=['TEMP'])\n",
        "    label_df = pd.DataFrame(labels, columns=['label'])\n",
        "    resp_df = pd.DataFrame(e4_data_dict['Resp'], columns=['Resp'])\n",
        "\n",
        "    # Filter EDA\n",
        "    eda_df['EDA'] = lowpass_filter(eda_df['EDA'], 1.0, smp_frq_dt['EDA'], 6)\n",
        "\n",
        "    # Filter ACM\n",
        "    for _ in acc_df.columns:\n",
        "        acc_df[_] = filterSignal(acc_df.values)\n",
        "\n",
        "    # Adding indices for combination due to differing sampling frequencies\n",
        "    eda_df.index = [(1 / smp_frq_dt['EDA']) * i for i in range(len(eda_df))]\n",
        "    bvp_df.index = [(1 / smp_frq_dt['BVP']) * i for i in range(len(bvp_df))]\n",
        "    acc_df.index = [(1 / smp_frq_dt['ACC']) * i for i in range(len(acc_df))]\n",
        "    temp_df.index = [(1 / smp_frq_dt['TEMP']) * i for i in range(len(temp_df))]\n",
        "    label_df.index = [(1 / smp_frq_dt['label']) * i for i in range(len(label_df))]\n",
        "    resp_df.index = [(1 / smp_frq_dt['Resp']) * i for i in range(len(resp_df))]\n",
        "\n",
        "    # Change indices to datetime\n",
        "    eda_df.index = pd.to_datetime(eda_df.index, unit='s')\n",
        "    bvp_df.index = pd.to_datetime(bvp_df.index, unit='s')\n",
        "    temp_df.index = pd.to_datetime(temp_df.index, unit='s')\n",
        "    acc_df.index = pd.to_datetime(acc_df.index, unit='s')\n",
        "    label_df.index = pd.to_datetime(label_df.index, unit='s')\n",
        "    resp_df.index = pd.to_datetime(resp_df.index, unit='s')\n",
        "\n",
        "    # New EDA features\n",
        "    r, p, t, l, d, e, obj = eda_stats(eda_df['EDA'])\n",
        "    eda_df['EDA_phasic'] = r\n",
        "    eda_df['EDA_smna'] = p\n",
        "    eda_df['EDA_tonic'] = t\n",
        "        \n",
        "    # Combined dataframe which is not used\n",
        "    df = eda_df.join(bvp_df, how='outer')\n",
        "    df = df.join(temp_df, how='outer')\n",
        "    df = df.join(acc_df, how='outer')\n",
        "    df = df.join(resp_df, how='outer')\n",
        "    df = df.join(label_df, how='outer')\n",
        "    df['label'] = df['label'].fillna(method='bfill')\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    if norm_type is 'std':\n",
        "        # std norm\n",
        "        df = (df - df.mean()) / df.std()\n",
        "    elif norm_type is 'minmax':\n",
        "        # minmax norm\n",
        "        df = (df - df.min()) / (df.max() - df.min())\n",
        "\n",
        "    # Groupby label\n",
        "    grouped = df.groupby('label')\n",
        "    baseline = grouped.get_group(1)\n",
        "    stress = grouped.get_group(2)\n",
        "    amusement = grouped.get_group(3)\n",
        "    return grouped, baseline, stress, amusement\n",
        "\n",
        "#Down sampling\n",
        "def get_samples(data, n_windows, label):\n",
        "    global feature\n",
        "    global WINDOW_IN_SECONDS\n",
        "\n",
        "    samples = []\n",
        "    window_len = smp_frq_dt['label'] * WINDOW_IN_SECONDS\n",
        "\n",
        "    for i in range(n_windows):\n",
        "        # Get window of data\n",
        "        w = data[window_len * i: window_len * (i + 1)]\n",
        "\n",
        "        # Add acc        \n",
        "        w = pd.concat([w, net_accel(w)])        \n",
        "        cols = list(w.columns)\n",
        "        cols[0] = 'net_acc'\n",
        "        w.columns = cols\n",
        "        \n",
        "        # Calculate statistics for window\n",
        "        wstats = compute_window_stats(data=w, label=label)\n",
        "\n",
        "        # Seperating sample and label\n",
        "        x = pd.DataFrame(wstats).drop('label', axis=0)\n",
        "        y = x['label'][0]\n",
        "        x.drop('label', axis=1, inplace=True)\n",
        "\n",
        "        if feature is None:\n",
        "            feature = []\n",
        "            for row in x.index:\n",
        "                for col in x.columns:\n",
        "                    feature.append('_'.join([row, col]))\n",
        "\n",
        "        # sample dataframe\n",
        "        wdf = pd.DataFrame(x.values.flatten()).T\n",
        "        wdf.columns = feature\n",
        "        wdf = pd.concat([wdf, pd.DataFrame({'label': y}, index=[0])], axis=1)\n",
        "        \n",
        "        # Additional features\n",
        "        wdf['BVP_peak_freq'] = peak_freq(w['BVP'].dropna())\n",
        "        wdf['TEMP_slope'] = get_slope(w['TEMP'].dropna())\n",
        "        samples.append(wdf)\n",
        "\n",
        "    return pd.concat(samples)\n",
        "\n",
        "\n",
        "def make_patient_data(subject_id):\n",
        "    global save_path\n",
        "    global WINDOW_IN_SECONDS\n",
        "\n",
        "    # Make subject data object for Sn\n",
        "    # Add your path to Dataset\n",
        "    subject = SubjectData(main_path='Path to Dataset', subject_number=subject_id)\n",
        "\n",
        "    # Empatica E4 data with RESP\n",
        "    e4_data_dict = subject.wrist_data()\n",
        "\n",
        "    norm_type = None\n",
        "\n",
        "    # Classifying Three Classes\n",
        "    grouped, baseline, stress, amusement = compute_features(e4_data_dict, subject.labels, norm_type)    \n",
        "    n_baseline_wdws = int(len(baseline) / (smp_frq_dt['label'] * WINDOW_IN_SECONDS))\n",
        "    n_stress_wdws = int(len(stress) / (smp_frq_dt['label'] * WINDOW_IN_SECONDS))\n",
        "    n_amusement_wdws = int(len(amusement) / (smp_frq_dt['label'] * WINDOW_IN_SECONDS))  \n",
        "    \n",
        "    # Downsampling\n",
        "    baseline_samples = get_samples(baseline, n_baseline_wdws, 1)\n",
        "    # baseline_samples = baseline_samples[::2]\n",
        "    stress_samples = get_samples(stress, n_stress_wdws, 2)\n",
        "    amusement_samples = get_samples(amusement, n_amusement_wdws, 0)\n",
        "\n",
        "    all_samples = pd.concat([baseline_samples, stress_samples, amusement_samples])\n",
        "    all_samples = pd.concat([all_samples.drop('label', axis=1), pd.get_dummies(all_samples['label'])], axis=1)    \n",
        "    all_samples.to_csv(f'{save_path}{sub_feature_path}/S{subject_id}_data.csv')    \n",
        "    subject = None\n",
        "\n",
        "# Combine Dataframes and Make CSV\n",
        "def combine_files(subjects):\n",
        "    df_list = []\n",
        "    for s in subjects:\n",
        "        df = pd.read_csv(f'{save_path}{sub_feature_path}/S{s}_data.csv', index_col=0)\n",
        "        df['subject'] = s\n",
        "        df_list.append(df)\n",
        "\n",
        "    df = pd.concat(df_list)\n",
        "\n",
        "    df['label'] = (df['0'].astype(str) + df['1'].astype(str) + df['2'].astype(str)).apply(lambda x: x.index('1'))\n",
        "    df.drop(['0', '1', '2'], axis=1, inplace=True)\n",
        "\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "    # Make csv\n",
        "    df.to_csv(f'{save_path}/stress.csv')\n",
        "\n",
        "    counts = df['label'].value_counts()\n",
        "    print('Number of samples per class:')\n",
        "    for label, number in zip(counts.index, counts.values):\n",
        "        print(f'{int_label[label]}: {number}')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    subject_ids = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17]\n",
        "\n",
        "    for patient in subject_ids:\n",
        "        print(f'Processing data for S{patient}...')\n",
        "        make_patient_data(patient)\n",
        "    #For combining Data\n",
        "    combine_files(subject_ids)\n",
        "    print('Processing complete.')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}